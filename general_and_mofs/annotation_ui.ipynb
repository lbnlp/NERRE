{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e07d7c",
   "metadata": {
    "id": "12e07d7c"
   },
   "source": [
    "# Annotation project workbook\n",
    "\n",
    "\n",
    "This notebook contains code for annotating materials science examples including (optional) in-the-loop annotation. Reasonable defaults as described in the publication are given, though these can be tweaked to user preference.\n",
    "\n",
    "Note: While we use OpenAI's API here for GPT-3 fine tuning, the openai code can be swapped out with handles to your LLM of choice.\n",
    "\n",
    "### Tips we've learned from annotation many thousands of examples:\n",
    "* You will encounter examples where it just isn't clear-cut how to fit the information from an abstract into the schema. Don't sweat these. Just do your best to do a \"good enough\" job and move on. These \"bad/ok\" examples will not affect the overall training much but it's important the model has seen this kind of thing before we run it over the full dataset.\n",
    "* Don't succumb to sunk-cost fallacy. If the schema needs to change, do it as soon as you know in your heart it probably should change.\n",
    "* We are aiming for about 1,000 annotations in total. After each round of 100 new annotations, retrain the model and use a new checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V2lk7a1Yk6gv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2lk7a1Yk6gv",
    "outputId": "0dce0404-c4f6-46b8-877d-4f79d134b732"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19fb1991",
   "metadata": {
    "id": "19fb1991"
   },
   "source": [
    "## Step 1. Definining a schema\n",
    "\n",
    "Use this section to define your annotation schema. It should be a python dictionary (e.g. jsonable).\n",
    "\n",
    "Each distinct material mentioned in an abstract should get it's own entry and the result of an annotation \"submit\" will be a list of these dictionaries in the order they're mentioned in the text.\n",
    "\n",
    "Here is an example we use for general materials data extraction:\n",
    "```\n",
    "material_schema = {\n",
    "    'name': '',\n",
    "    'formula': '',\n",
    "    'description': [''],\n",
    "    'acronym': '',\n",
    "    'structure_or_phase': [''],\n",
    "    'applications': ['']\n",
    "    }\n",
    "```\n",
    "\n",
    "**Note: You can replace this schema with your own as you see fit (as long as the schema is a json-type document)**. If you use in the loop annotation, you should make sure the model you are using as the intermediate model is trained on the same schema you define here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868745d6",
   "metadata": {
    "id": "868745d6"
   },
   "outputs": [],
   "source": [
    "my_schema = {\n",
    "    'name': '',\n",
    "    'formula': '',\n",
    "    'description': [''],\n",
    "    'acronym': '',\n",
    "    'structure_or_phase': [''],\n",
    "    'applications': ['']\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d673e5",
   "metadata": {
    "id": "a5d673e5"
   },
   "source": [
    "## Step 2. Set up a query to get training examples\n",
    "\n",
    "This notebook queries [matscholar.com](matscholar.com) using the matscholar query syntax. You can use a simple phrase or some of the more advanced query operators. We advise you to build your query on matscholar.com first before pasting it here.\n",
    "\n",
    "\n",
    "In our example query, we use \"ferrimagnetic\". But feel free to choose your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac891969",
   "metadata": {
    "id": "ac891969"
   },
   "outputs": [],
   "source": [
    "MY_QUERY = \"ferrimagnetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f56df",
   "metadata": {
    "id": "881f56df"
   },
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87202bb9",
   "metadata": {
    "id": "87202bb9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def remove_many_newlines(string):\n",
    "    \"\"\"Removes all cases where there are multiple adjacent spaces after a newline in a string.\"\"\"\n",
    "    return re.sub(r'(\\\\n\\ +)', '', string)\n",
    "\n",
    "def remove_junk(s):\n",
    "    for junk in [\"<hi>\", \"</hi>\", \"<inf>\", \"</inf>\", \"<sup>\", \"</sup>\", \"</sub>\", \"</sub>\"]:\n",
    "        s = s.replace(junk, \"\")\n",
    "    if \"\\n  \" in s:\n",
    "        s = remove_many_newlines(s)\n",
    "    return s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c8c69cf",
   "metadata": {
    "id": "9c8c69cf"
   },
   "source": [
    "## Step 3: Get test data from matscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad6aa9",
   "metadata": {
    "id": "b1ad6aa9"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib import parse\n",
    "\n",
    "def query_matscholar(query, hits=100, exlude_dois=[]):\n",
    "    \"\"\"Submits a query to matscholar and returns up to 10000 results.\n",
    "    Args:\n",
    "        query (str): Matscholar query. Supports matscholar query syntax.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    search_uri = f\"https://matscholar.com/api/search/?query={parse.quote(query)}&type=all&restrict=doc&hits=1\"\n",
    "    num_results = requests.get(search_uri).json()['root']['fields']['totalCount']\n",
    "    num_results = min(num_results, hits)\n",
    "    for offset in range(0, num_results // 100 + 1):\n",
    "        offset*=100\n",
    "        search_uri = f\"https://matscholar.com/api/search/?query={parse.quote(query)}&offset={offset}&type=all&restrict=doc&hits={min(hits, 100)}\"\n",
    "        results = requests.get(search_uri)\n",
    "        if 'children' in results.json()['root']:\n",
    "            all_results.extend(results.json()['root']['children'])\n",
    "        else:\n",
    "            break\n",
    "    return [d for d in all_results if d['fields']['doi'] not in exlude_dois]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b548cb5",
   "metadata": {},
   "source": [
    "To use in-the-loop annotation with your own model, specify the API key and your model name. The annotation UI will automatically query the API to get a \"good guess\" for a completion based on your model. The quality of your model dertermines the quality of the guess. From this guess, you'll correct the annotation in the UI. We find this method to be much faster than annotating all abstracts from scratch. Usually, you'll need around 50 abstracts to train a decent intermediate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390191f",
   "metadata": {
    "id": "e390191f"
   },
   "outputs": [],
   "source": [
    "# create a completion\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "YOUR_MODEL_NAME = \"YOUR_MODEL_NAME_HERE\"\n",
    "\n",
    "def extract_materials_data(abstract, model=YOUR_MODEL_NAME):\n",
    "    \"\"\" Sends abstract to OpenAI API and uses custom model name if provided.\n",
    "    \"\"\"\n",
    "    if model and model != \"YOUR_MODEL_NAME_HERE\":\n",
    "        start_sequence = \"\\n\\n###\\n\\n\"\n",
    "        prompt = abstract + start_sequence\n",
    "        response = openai.Completion.create(\n",
    "          model = model,\n",
    "          prompt=prompt,\n",
    "          temperature=0,\n",
    "          max_tokens=512,\n",
    "          top_p=1,\n",
    "          frequency_penalty=0,\n",
    "          presence_penalty=0,\n",
    "          stop=[\"\\n\\nEND\\n\\n\"],\n",
    "        )\n",
    "        return response.choices[0].text.replace(\"name_of_mof\", \"mof_name\")\n",
    "    else:\n",
    "        return json.dumps([my_schema])\n",
    "\n",
    "\n",
    "def clean_up_results(results, fields_to_keep=['abstract', 'title', 'doi', 'year']):\n",
    "    cleaned_results = []\n",
    "    for r in results:\n",
    "        new_entry = {key:r['fields'][key] for key in fields_to_keep}\n",
    "        new_entry['title'] = remove_junk(new_entry['title'])\n",
    "        new_entry['abstract'] = remove_junk(new_entry['abstract'])\n",
    "        new_entry['annotation'] = []\n",
    "        cleaned_results.append(new_entry)\n",
    "    return cleaned_results\n",
    "\n",
    "def make_prompt(entry):\n",
    "    return entry['title'] + \"\\n\" + entry['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7bf11",
   "metadata": {
    "id": "d5a7bf11"
   },
   "source": [
    "### Load previous annotations to prevent getting same paper twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758ab5c",
   "metadata": {
    "id": "b758ab5c"
   },
   "outputs": [],
   "source": [
    "PREVIOUS_ANNOTATIONS = \"\" # e.g. \"/content/drive/MyDrive/<path to previous annotations folder>\"\n",
    "ANNOTATIONS_PREFIX = \"my_annotations\" # prefix for annotation files\n",
    "if PREVIOUS_ANNOTATIONS:\n",
    "    with open(PREVIOUS_ANNOTATIONS, \"r\") as file:\n",
    "        if PREVIOUS_ANNOTATIONS.endswith(\"jsonl\"):\n",
    "            lines = file.readlines()\n",
    "            records = []\n",
    "            for line in lines:\n",
    "                records.append(json.loads(line))\n",
    "        else:\n",
    "            records = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad08b554",
   "metadata": {
    "id": "ad08b554"
   },
   "source": [
    "\n",
    "## Step 4: Annotate!\n",
    "### Run the annotation GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf1853",
   "metadata": {
    "id": "2bbf1853",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "results = clean_up_results(query_matscholar(MY_QUERY, hits=900))\n",
    "results = random.sample(results, 100)\n",
    "\n",
    "try:\n",
    "    previous_prompts = [x['prompt'] for x in records]\n",
    "except:\n",
    "    previous_prompts = []\n",
    "queue = [r for r in results if make_prompt(r) not in previous_prompts]\n",
    "saved_entries = []\n",
    "\n",
    "material_schema = my_schema\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# button to add new material\n",
    "new_material_button = widgets.Button(\n",
    "    description='Add Material',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Add new material',\n",
    "    icon='plus' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_new_material_button_clicked(b):\n",
    "    global entry\n",
    "    entry['annotation'] = eval(text_area.value)\n",
    "    entry['annotation'].append(material_schema.copy())\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        text_area.value = json.dumps(entry['annotation'], indent=2)\n",
    "        display(content)\n",
    "\n",
    "new_material_button.on_click(on_new_material_button_clicked)\n",
    "\n",
    "\n",
    "back_button = widgets.Button(\n",
    "    description='Back',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    icon='back' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_back_button_clicked(b):\n",
    "    global entry\n",
    "    global queue\n",
    "    queue.append(entry)\n",
    "    entry = saved_entries.pop()\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        gui_abstract.value = make_prompt(entry)\n",
    "        text_area.value = json.dumps(entry['annotation'], indent=2)\n",
    "        display(content)\n",
    "\n",
    "back_button.on_click(on_back_button_clicked)\n",
    "\n",
    "# save annotation\n",
    "save_button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save annotation',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_save_button_clicked(b):\n",
    "    global entry\n",
    "    global queue\n",
    "    entry['annotation'] = eval(text_area.value)\n",
    "    with output:\n",
    "        gui_abstract.value = \"Please wait...\"\n",
    "        text_area.value = \"Please wait...\"\n",
    "    saved_entries.append(entry)\n",
    "    entry = queue.pop()\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        prompt = make_prompt(entry)\n",
    "        entry['annotation'] = json.loads(extract_materials_data(prompt))\n",
    "        entry['annotation'] = entry['annotation']\n",
    "        gui_abstract.value = prompt\n",
    "        text_area.value = json.dumps(entry['annotation'], indent=2)\n",
    "        display(content)\n",
    "\n",
    "save_button.on_click(on_save_button_clicked)\n",
    "\n",
    "\n",
    "# skip annotation\n",
    "skip_button = widgets.Button(\n",
    "    description='Skip',\n",
    "    disabled=False,\n",
    "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save annotation',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_skip_button_clicked(b):\n",
    "    global entry\n",
    "    global queue\n",
    "    entry = queue.pop()\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        prompt = make_prompt(entry)\n",
    "        entry['annotation'] = json.loads(extract_materials_data(prompt))\n",
    "        entry['annotation'] = entry['annotation']\n",
    "        gui_abstract.value = prompt\n",
    "        text_area.value = json.dumps(entry['annotation'], indent=2)\n",
    "        display(content)\n",
    "\n",
    "skip_button.on_click(on_skip_button_clicked)\n",
    "\n",
    "# clear annotation\n",
    "clear_button = widgets.Button(\n",
    "    description='Clear',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Clear annotation',\n",
    ")\n",
    "\n",
    "def on_clear_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        text_area.value = json.dumps([], indent=2)\n",
    "        display(content)\n",
    "\n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "\n",
    "def setup():\n",
    "    global entry\n",
    "    global queue\n",
    "    entry = queue.pop()\n",
    "    entry['annotation'] = json.loads(extract_materials_data(make_prompt(entry)))\n",
    "    entry['annotation'] = entry['annotation']\n",
    "\n",
    "    text_area = widgets.Textarea(value=json.dumps(entry['annotation'], indent=2),\n",
    "                                 layout=widgets.Layout(height='600px'))\n",
    "    gui_abstract = widgets.Textarea(disabled=True,\n",
    "                                    value=make_prompt(entry),\n",
    "                                    layout=widgets.Layout(width='70%', height='200px'))\n",
    "    return text_area, gui_abstract\n",
    "\n",
    "text_area, gui_abstract = setup()\n",
    "\n",
    "content = widgets.VBox([\n",
    "    widgets.HBox(\n",
    "        [save_button,skip_button, back_button]),\n",
    "    gui_abstract,\n",
    "    widgets.HBox(\n",
    "        [new_material_button, clear_button]),\n",
    "    text_area]\n",
    ")\n",
    "display(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "585d8622",
   "metadata": {
    "id": "585d8622"
   },
   "source": [
    "### Saving your annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a34c2c",
   "metadata": {
    "id": "f0a34c2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for entry in saved_entries:\n",
    "    records.append({'prompt':make_prompt(entry), 'completion':entry['annotation'], 'record':entry})\n",
    "print(\"You have\", len(records), \"records to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4b9ab",
   "metadata": {
    "id": "10c4b9ab"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def save_records(records, combine_with=None):\n",
    "    old_records = []\n",
    "    if combine_with:\n",
    "        try:\n",
    "            with open(combine_with, \"r\") as file:\n",
    "                old_records = json.load(file)\n",
    "\n",
    "        except:\n",
    "            with open(combine_with, \"r\") as file:\n",
    "                old_records = [json.loads(f) for f in file.readlines()]\n",
    "    clean_records = old_records + [record for record in records if record not in old_records]\n",
    "    filename = f\"{ANNOTATIONS_PREFIX}_{datetime.now().strftime('%m_%d_%Y_%H%M%S')}.json\"\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(clean_records, file)\n",
    "    return filename\n",
    "\n",
    "records_filepath = save_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55661982",
   "metadata": {
    "id": "55661982"
   },
   "source": [
    "## Loading previous annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d78f27",
   "metadata": {
    "id": "e7d78f27"
   },
   "outputs": [],
   "source": [
    "def load_records(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "records = load_records(records_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f53031e",
   "metadata": {},
   "source": [
    "Now we prepare a jsonlines file for input to the OpenAI API (or LLM of your chocie.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05508ae6",
   "metadata": {
    "id": "05508ae6"
   },
   "outputs": [],
   "source": [
    "def prepare_fine_tune(filename):\n",
    "    new_filename = filename.replace(\".json\", \".jsonl\")\n",
    "    with open(new_filename, \"w\") as writer:\n",
    "        for r in records:\n",
    "            r_new = {}\n",
    "            r_new['prompt'] = r['prompt'] + \"\\n\\n###\\n\\n\"\n",
    "            r_new['completion'] = ' ' + json.dumps(r['completion']) + '\\n\\nEND\\n\\n'\n",
    "            writer.write(json.dumps(r_new) + \"\\n\")\n",
    "    print(f\"JSONL file written to {new_filename}\")\n",
    "\n",
    "prepare_fine_tune(records_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "VCAFt9VPu_t_",
   "metadata": {
    "id": "VCAFt9VPu_t_"
   },
   "source": [
    "## Step 5: Train model\n",
    "\n",
    "Now in a terminal, use the prepared file to train a model through the openai api.\n",
    "\n",
    "The exact syntax for this depends on the version of the openai python package you are using, the train/validation/test split you desire, and other factors.\n",
    "\n",
    "For more info, see the OpenAI API docs or use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: openai api fine_tunes.create [-h] -t TRAINING_FILE [-v VALIDATION_FILE]\r\n",
      "                                    [--no_check_if_files_exist] [-m MODEL]\r\n",
      "                                    [--suffix SUFFIX] [--no_follow]\r\n",
      "                                    [--n_epochs N_EPOCHS]\r\n",
      "                                    [--batch_size BATCH_SIZE]\r\n",
      "                                    [--learning_rate_multiplier LEARNING_RATE_MULTIPLIER]\r\n",
      "                                    [--prompt_loss_weight PROMPT_LOSS_WEIGHT]\r\n",
      "                                    [--compute_classification_metrics]\r\n",
      "                                    [--classification_n_classes CLASSIFICATION_N_CLASSES]\r\n",
      "                                    [--classification_positive_class CLASSIFICATION_POSITIVE_CLASS]\r\n",
      "                                    [--classification_betas CLASSIFICATION_BETAS [CLASSIFICATION_BETAS ...]]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -t TRAINING_FILE, --training_file TRAINING_FILE\r\n",
      "                        JSONL file containing prompt-completion examples for\r\n",
      "                        training. This can be the ID of a file uploaded\r\n",
      "                        through the OpenAI API (e.g. file-abcde12345), a local\r\n",
      "                        file path, or a URL that starts with \"http\".\r\n",
      "  -v VALIDATION_FILE, --validation_file VALIDATION_FILE\r\n",
      "                        JSONL file containing prompt-completion examples for\r\n",
      "                        validation. This can be the ID of a file uploaded\r\n",
      "                        through the OpenAI API (e.g. file-abcde12345), a local\r\n",
      "                        file path, or a URL that starts with \"http\".\r\n",
      "  --no_check_if_files_exist\r\n",
      "                        If this argument is set and training_file or\r\n",
      "                        validation_file are file paths, immediately upload\r\n",
      "                        them. If this argument is not set, check if they may\r\n",
      "                        be duplicates of already uploaded files before\r\n",
      "                        uploading, based on file name and file size.\r\n",
      "  -m MODEL, --model MODEL\r\n",
      "                        The model to start fine-tuning from\r\n",
      "  --suffix SUFFIX       If set, this argument can be used to customize the\r\n",
      "                        generated fine-tuned model name.All punctuation and\r\n",
      "                        whitespace in `suffix` will be replaced with a single\r\n",
      "                        dash, and the string will be lower cased. The max\r\n",
      "                        length of `suffix` is 40 chars. The generated name\r\n",
      "                        will match the form `{base_model}:ft-{org-\r\n",
      "                        title}:{suffix}-{timestamp}`. For example, `openai api\r\n",
      "                        fine_tunes.create -t test.jsonl -m ada --suffix\r\n",
      "                        \"custom model name\" could generate a model with the\r\n",
      "                        name ada:ft-your-org:custom-model-\r\n",
      "                        name-2022-02-15-04-21-04\r\n",
      "  --no_follow           If set, returns immediately after creating the job.\r\n",
      "                        Otherwise, streams events and waits for the job to\r\n",
      "                        complete.\r\n",
      "  --n_epochs N_EPOCHS   The number of epochs to train the model for. An epoch\r\n",
      "                        refers to one full cycle through the training dataset.\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "                        The batch size to use for training. The batch size is\r\n",
      "                        the number of training examples used to train a single\r\n",
      "                        forward and backward pass.\r\n",
      "  --learning_rate_multiplier LEARNING_RATE_MULTIPLIER\r\n",
      "                        The learning rate multiplier to use for training. The\r\n",
      "                        fine-tuning learning rate is determined by the\r\n",
      "                        original learning rate used for pretraining multiplied\r\n",
      "                        by this value.\r\n",
      "  --prompt_loss_weight PROMPT_LOSS_WEIGHT\r\n",
      "                        The weight to use for the prompt loss. The optimum\r\n",
      "                        value here depends depends on your use case. This\r\n",
      "                        determines how much the model prioritizes learning\r\n",
      "                        from prompt tokens vs learning from completion tokens.\r\n",
      "  --compute_classification_metrics\r\n",
      "                        If set, we calculate classification-specific metrics\r\n",
      "                        such as accuracy and F-1 score using the validation\r\n",
      "                        set at the end of every epoch.\r\n",
      "  --classification_n_classes CLASSIFICATION_N_CLASSES\r\n",
      "                        The number of classes in a classification task. This\r\n",
      "                        parameter is required for multiclass classification.\r\n",
      "  --classification_positive_class CLASSIFICATION_POSITIVE_CLASS\r\n",
      "                        The positive class in binary classification. This\r\n",
      "                        parameter is needed to generate precision, recall and\r\n",
      "                        F-1 metrics when doing binary classification.\r\n",
      "  --classification_betas CLASSIFICATION_BETAS [CLASSIFICATION_BETAS ...]\r\n",
      "                        If this is provided, we calculate F-beta scores at the\r\n",
      "                        specified beta values. The F-beta score is a\r\n",
      "                        generalization of F-1 score. This is only used for\r\n",
      "                        binary classification.\r\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create --help"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T08:51:13.595823Z",
     "start_time": "2023-07-17T08:51:10.129865Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0532e16c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nerre_official_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
